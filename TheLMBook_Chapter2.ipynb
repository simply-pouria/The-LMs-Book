{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6M2h3VqnuQRafcQZK3xMZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simply-pouria/The-LMs-Book/blob/main/TheLMBook_Chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2\n",
        "### Training a Bag-of-Words Neural Network\n",
        "using the sofmax activation function and cross-entropy loss function\n"
      ],
      "metadata": {
        "id": "c0VyqBCdg72R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qppRH29kpfHo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.manual_seed(42)\n",
        "\n",
        "docs = [\n",
        " \"Movies are fun for everyone.\",\n",
        " \"Watching movies is great fun.\",\n",
        " \"Enjoy a great movie today.\",\n",
        " \"Research is interesting and important.\",\n",
        " \"Learning math is very important.\",\n",
        " \"Science discovery is interesting.\",\n",
        " \"Rock is great to listen to.\",\n",
        " \"Listen to music for fun.\",\n",
        " \"Music is fun for everyone.\",\n",
        " \"Listen to folk music!\"\n",
        "]\n",
        "labels = [1, 1, 1, 3, 3, 3, 2, 2, 2, 2]\n",
        "num_classes = len(set(labels))\n",
        "\n",
        "def tokenize(text):\n",
        "  return re.findall(r\"\\w+\", text.lower())\n",
        "def get_vocabulary(texts):\n",
        "  tokens = {token for text in texts for token in tokenize(text)}\n",
        "  return {word: idx for idx, word in enumerate(sorted(tokens))}\n",
        "\n",
        "vocabulary = get_vocabulary(docs)\n",
        "\n",
        "def doc_to_bow(doc, vocabulary):\n",
        "  tokens = set(tokenize(doc))\n",
        "  bow = [0] * len(vocabulary)\n",
        "  for token in tokens:\n",
        "    if token in vocabulary:\n",
        "      bow[vocabulary[token]] = 1\n",
        "      return bow\n",
        "\n",
        "vectors = torch.tensor(\n",
        " [doc_to_bow(doc, vocabulary) for doc in docs],\n",
        " dtype=torch.float32\n",
        ")\n",
        "labels = torch.tensor(labels, dtype=torch.long) - 1\n",
        "\n",
        "input_dim = len(vocabulary)\n",
        "hidden_dim = 50\n",
        "output_dim = num_classes\n",
        "\n",
        "class SimpleClassifier(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "model = SimpleClassifier(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for step in range(3000):\n",
        "  optimizer.zero_grad()\n",
        "  loss = criterion(model(vectors), labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inferencing the Trained Model"
      ],
      "metadata": {
        "id": "3mgyNa5-g66p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_docs = [\n",
        " \"Listening to rock music is fun.\",\n",
        " \"I like to listen to this song.\"\n",
        "]\n",
        "class_names = [\"Cinema\", \"Music\", \"Science\"]\n",
        "new_doc_vectors = torch.tensor(\n",
        " [doc_to_bow(new_doc, vocabulary) for new_doc in new_docs]\n",
        ",\n",
        " dtype=torch.float32\n",
        ")\n",
        "with torch.no_grad():\n",
        " outputs = model(new_doc_vectors)\n",
        " predicted_ids = torch.argmax(outputs, dim=1) + 1\n",
        "for i, new_doc in enumerate(new_docs):\n",
        " print(f'{new_doc}: {class_names[predicted_ids[i].item() -1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY2IWJkkpihx",
        "outputId": "48f8815e-afb9-43f3-e830-0a046cb07ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listening to rock music is fun.: Music\n",
            "I like to listen to this song.: Music\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementning Byte-Pair Encoding\n"
      ],
      "metadata": {
        "id": "y3Jj9Szqhajm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def initialize_vocabulary(corpus):\n",
        "  vocabulary = defaultdict(int)\n",
        "  charset = set()\n",
        "  for word in corpus:\n",
        "    word_with_marker = '_' + word\n",
        "    characters = list(word_with_marker)\n",
        "    charset.update(characters)\n",
        "    tokenized_word = ' '.join(characters)\n",
        "    vocabulary[tokenized_word] += 1\n",
        "  return vocabulary, charset\n",
        "\n",
        "\n",
        "def get_pair_counts(vocabulary):\n",
        "  pair_counts = defaultdict(int)\n",
        "  for tokenized_word, count in vocabulary.items():\n",
        "    tokens = tokenized_word.split()\n",
        "    for i in range(len(tokens) - 1):\n",
        "      pair = (tokens[i], tokens[i + 1])\n",
        "      pair_counts[pair] += count\n",
        "  return pair_counts\n",
        "\n",
        "\n",
        "def merge_pair(vocabulary, pair):\n",
        "  new_vocabulary = {}\n",
        "  bigram = re.escape(' '.join(pair))\n",
        "  pattern = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
        "  for tokenized_word, count in vocabulary.items():\n",
        "    new_tokenized_word = pattern.sub(\"\".join(pair), tokenized_word)\n",
        "    new_vocabulary[new_tokenized_word] = count\n",
        "  return new_vocabulary\n",
        "\n",
        "\n",
        "def byte_pair_encoding(corpus, vocab_size):\n",
        "  vocabulary, charset = initialize_vocabulary(corpus)\n",
        "  merges = []\n",
        "  tokens = set(charset)\n",
        "  while len(tokens) < vocab_size:\n",
        "    pair_counts = get_pair_counts(vocabulary)\n",
        "    if not pair_counts:\n",
        "     break\n",
        "    most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
        "    merges.append(most_frequent_pair)\n",
        "    vocabulary = merge_pair(vocabulary, most_frequent_pair)\n",
        "    new_token = ''.join(most_frequent_pair)\n",
        "    tokens.add(new_token)\n",
        "\n",
        "  return vocabulary, merges, charset, tokens\n",
        "\n",
        "def tokenize_word(word, merges, vocabulary, charset, unk_token=\"<UNK>\"):\n",
        "  word = '_' + word\n",
        "  if word in vocabulary:\n",
        "    return [word]\n",
        "  tokens = [char if char in charset else unk_token for char in word]\n",
        "\n",
        "  for left, right in merges:\n",
        "    i = 0\n",
        "    while i < len(tokens) - 1:\n",
        "      if tokens[i:i+2] == [left, right]:\n",
        "        tokens[i:i+2] = [left + right]\n",
        "      else:\n",
        "        i += 1\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "O4Plsz61h_eh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a Count-Based Language Model"
      ],
      "metadata": {
        "id": "9QT925fESgjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining The model\n"
      ],
      "metadata": {
        "id": "h2oWZR5vWs91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CountLanguageModel:\n",
        "  def __init__(self, n):\n",
        "    self.n = n\n",
        "    self.ngram_counts = [{} for _ in range(n)]\n",
        "    self.total_unigrams = 0\n",
        "  def predict_next_token(self, context):\n",
        "    for n in range(self.n, 1, -1):\n",
        "      if len(context) >= n - 1:\n",
        "        context_n = tuple(context[-(n - 1):])\n",
        "        counts = self.ngram_counts[n - 1].get(context_n)\n",
        "      if counts:\n",
        "        return max(counts.items(), key=lambda x: x[1])[0]\n",
        "    unigram_counts = self.ngram_counts[0].get(())\n",
        "    if unigram_counts:\n",
        "      return max(unigram_counts.items(), key=lambda x: x[1])[0]\n",
        "    return None"
      ],
      "metadata": {
        "id": "ODOLeMWrSq0u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "bs_ULes5qYZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import re         # For regular expressions (text tokenization)\n",
        "import requests   # For downloading the corpus\n",
        "import gzip       # For decompressing the downloaded corpus\n",
        "import io         # For handling byte streams\n",
        "import math       # For mathematical operations (log, exp)\n",
        "import random     # For random number generation\n",
        "from collections import defaultdict  # For efficient dictionary operations\n",
        "import pickle, os # For saving and loading the model\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Sets random seeds for reproducibility.\n",
        "\n",
        "    Args:\n",
        "        seed (int): Seed value for the random number generator\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "def download_corpus(url):\n",
        "    \"\"\"\n",
        "    Downloads and decompresses a gzipped corpus file from the given URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL of the gzipped corpus file\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded text content of the corpus\n",
        "\n",
        "    Raises:\n",
        "        HTTPError: If the download fails\n",
        "    \"\"\"\n",
        "    print(f\"Downloading corpus from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raises an exception for bad HTTP responses\n",
        "\n",
        "    print(\"Decompressing and reading the corpus...\")\n",
        "    with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as f:\n",
        "        corpus = f.read().decode('utf-8')\n",
        "\n",
        "    print(f\"Corpus size: {len(corpus)} characters\")\n",
        "    return corpus\n",
        "\n",
        "class CountLanguageModel:\n",
        "    \"\"\"\n",
        "    Implements an n-gram language model using count-based probability estimation.\n",
        "    Supports variable context lengths up to n-grams.\n",
        "    \"\"\"\n",
        "    def __init__(self, n):\n",
        "        \"\"\"\n",
        "        Initialize the model with maximum n-gram length.\n",
        "\n",
        "        Args:\n",
        "            n (int): Maximum length of n-grams to use\n",
        "        \"\"\"\n",
        "        self.n = n  # Maximum n-gram length\n",
        "        self.ngram_counts = [{} for _ in range(n)]  # List of dictionaries for each n-gram length\n",
        "        self.total_unigrams = 0  # Total number of tokens in training data\n",
        "\n",
        "    def predict_next_token(self, context):\n",
        "        \"\"\"\n",
        "        Predicts the most likely next token given a context.\n",
        "        Uses backoff strategy: tries largest n-gram first, then backs off to smaller n-grams.\n",
        "\n",
        "        Args:\n",
        "            context (list): List of tokens providing context for prediction\n",
        "\n",
        "        Returns:\n",
        "            str: Most likely next token, or None if no prediction can be made\n",
        "        \"\"\"\n",
        "        for n in range(self.n, 1, -1):  # Start with largest n-gram, back off to smaller ones\n",
        "            if len(context) >= n - 1:\n",
        "                context_n = tuple(context[-(n - 1):])  # Get the relevant context for this n-gram\n",
        "                counts = self.ngram_counts[n - 1].get(context_n)\n",
        "                if counts:\n",
        "                    return max(counts.items(), key=lambda x: x[1])[0]  # Return most frequent token\n",
        "        # Backoff to unigram if no larger context matches\n",
        "        unigram_counts = self.ngram_counts[0].get(())\n",
        "        if unigram_counts:\n",
        "            return max(unigram_counts.items(), key=lambda x: x[1])[0]\n",
        "        return None\n",
        "\n",
        "    def get_probability(self, token, context):\n",
        "        for n in range(self.n, 1, -1):\n",
        "            if len(context) >= n - 1:\n",
        "                context_n = tuple(context[-(n - 1):])\n",
        "                counts = self.ngram_counts[n - 1].get(context_n)\n",
        "                if counts:\n",
        "                    total = sum(counts.values())\n",
        "                    count = counts.get(token, 0)\n",
        "                    if count > 0:\n",
        "                        return count / total\n",
        "        unigram_counts = self.ngram_counts[0].get(())\n",
        "        count = unigram_counts.get(token, 0)\n",
        "        V = len(unigram_counts)\n",
        "        return (count + 1) / (self.total_unigrams + V)\n",
        "\n",
        "def train(model, tokens):\n",
        "    \"\"\"\n",
        "    Trains the language model by counting n-grams in the training data.\n",
        "\n",
        "    Args:\n",
        "        model (CountLanguageModel): Model to train\n",
        "        tokens (list): List of tokens from the training corpus\n",
        "    \"\"\"\n",
        "    # Train models for each n-gram size from 1 to n\n",
        "    for n in range(1, model.n + 1):\n",
        "        counts = model.ngram_counts[n - 1]\n",
        "        # Slide a window of size n over the corpus\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            # Split into context (n-1 tokens) and next token\n",
        "            context = tuple(tokens[i:i + n - 1])\n",
        "            next_token = tokens[i + n - 1]\n",
        "\n",
        "            # Initialize counts dictionary for this context if needed\n",
        "            if context not in counts:\n",
        "                counts[context] = defaultdict(int)\n",
        "\n",
        "            # Increment count for this context-token pair\n",
        "            counts[context][next_token] = counts[context][next_token] + 1\n",
        "\n",
        "    # Store total number of tokens for unigram probability calculations\n",
        "    model.total_unigrams = len(tokens)\n",
        "\n",
        "def generate_text(model, context, num_tokens):\n",
        "    \"\"\"\n",
        "    Generates text by repeatedly sampling from the model.\n",
        "\n",
        "    Args:\n",
        "        model (CountLanguageModel): Trained language model\n",
        "        context (list): Initial context tokens\n",
        "        num_tokens (int): Number of tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text including initial context\n",
        "    \"\"\"\n",
        "    # Start with the provided context\n",
        "    generated = list(context)\n",
        "\n",
        "    # Generate new tokens until we reach the desired length\n",
        "    while len(generated) - len(context) < num_tokens:\n",
        "        # Use the last n-1 tokens as context for prediction\n",
        "        next_token = model.predict_next_token(generated[-(model.n-1):])\n",
        "        generated.append(next_token)\n",
        "\n",
        "        # Stop if we've generated enough tokens AND found a period\n",
        "        # This helps ensure complete sentences\n",
        "        if len(generated) - len(context) >= num_tokens and next_token == '.':\n",
        "            break\n",
        "\n",
        "    # Join tokens with spaces to create readable text\n",
        "    return ' '.join(generated)\n",
        "\n",
        "def compute_perplexity(model, tokens, context_size):\n",
        "    \"\"\"\n",
        "    Computes perplexity of the model on given tokens.\n",
        "\n",
        "    Args:\n",
        "        model (CountLanguageModel): Trained language model\n",
        "        tokens (list): List of tokens to evaluate on\n",
        "        context_size (int): Maximum context size to consider\n",
        "\n",
        "    Returns:\n",
        "        float: Perplexity score (lower is better)\n",
        "    \"\"\"\n",
        "    # Handle empty token list\n",
        "    if not tokens:\n",
        "        return float('inf')\n",
        "\n",
        "    # Initialize log likelihood accumulator\n",
        "    total_log_likelihood = 0\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    # Calculate probability for each token given its context\n",
        "    for i in range(num_tokens):\n",
        "        # Get appropriate context window, handling start of sequence\n",
        "        context_start = max(0, i - context_size)\n",
        "        context = tuple(tokens[context_start:i])\n",
        "        token = tokens[i]\n",
        "\n",
        "        # Get probability of this token given its context\n",
        "        probability = model.get_probability(token, context)\n",
        "\n",
        "        # Add log probability to total (using log for numerical stability)\n",
        "        total_log_likelihood += math.log(probability)\n",
        "\n",
        "    # Calculate average log likelihood\n",
        "    average_log_likelihood = total_log_likelihood / num_tokens\n",
        "\n",
        "    # Convert to perplexity: exp(-average_log_likelihood)\n",
        "    # Lower perplexity indicates better model performance\n",
        "    perplexity = math.exp(-average_log_likelihood)\n",
        "    return perplexity\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes text into words and periods.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to tokenize\n",
        "\n",
        "    Returns:\n",
        "        list: List of lowercase tokens matching words or periods\n",
        "    \"\"\"\n",
        "    return re.findall(r\"\\b[a-zA-Z0-9]+\\b|[.]\", text.lower())\n",
        "\n",
        "def download_and_prepare_data(data_url):\n",
        "    \"\"\"\n",
        "    Downloads and prepares training and test data.\n",
        "\n",
        "    Args:\n",
        "        data_url (str): URL of the corpus to download\n",
        "\n",
        "    Returns:\n",
        "        tuple: (training_tokens, test_tokens) split 90/10\n",
        "    \"\"\"\n",
        "    # Download and extract the corpus\n",
        "    corpus = download_corpus(data_url)\n",
        "\n",
        "    # Convert text to tokens\n",
        "    tokens = tokenize(corpus)\n",
        "\n",
        "    # Split into training (90%) and test (10%) sets\n",
        "    split_index = int(len(tokens) * 0.9)\n",
        "    train_corpus = tokens[:split_index]\n",
        "    test_corpus = tokens[split_index:]\n",
        "\n",
        "    return train_corpus, test_corpus\n",
        "\n",
        "def save_model(model, model_name):\n",
        "    \"\"\"\n",
        "    Saves the trained language model to disk.\n",
        "\n",
        "    Args:\n",
        "        model (CountLanguageModel): Trained model to save\n",
        "        model_name (str): Name to use for the saved model file\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the saved model file\n",
        "\n",
        "    Raises:\n",
        "        IOError: If there's an error writing to disk\n",
        "    \"\"\"\n",
        "    # Create models directory if it doesn't exist\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "\n",
        "    # Construct file path\n",
        "    model_path = os.path.join('models', f'{model_name}.pkl')\n",
        "\n",
        "    try:\n",
        "        print(f\"Saving model to {model_path}...\")\n",
        "        with open(model_path, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'n': model.n,\n",
        "                'ngram_counts': model.ngram_counts,\n",
        "                'total_unigrams': model.total_unigrams\n",
        "            }, f)\n",
        "        print(\"Model saved successfully.\")\n",
        "        return model_path\n",
        "    except IOError as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_model(model_name):\n",
        "    \"\"\"\n",
        "    Loads a trained language model from disk.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the model to load\n",
        "\n",
        "    Returns:\n",
        "        CountLanguageModel: Loaded model instance\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the model file doesn't exist\n",
        "        IOError: If there's an error reading the file\n",
        "    \"\"\"\n",
        "    model_path = os.path.join('models', f'{model_name}.pkl')\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading model from {model_path}...\")\n",
        "        with open(model_path, 'rb') as f:\n",
        "            model_data = pickle.load(f)\n",
        "\n",
        "        # Create new model instance\n",
        "        model = CountLanguageModel(model_data['n'])\n",
        "\n",
        "        # Restore model state\n",
        "        model.ngram_counts = model_data['ngram_counts']\n",
        "        model.total_unigrams = model_data['total_unigrams']\n",
        "\n",
        "        print(\"Model loaded successfully.\")\n",
        "        return model\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Model file not found: {model_path}\")\n",
        "        raise\n",
        "    except IOError as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        raise\n",
        "\n",
        "def get_hyperparameters():\n",
        "    \"\"\"\n",
        "    Returns model hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        int: Size of n-grams to use in the model\n",
        "    \"\"\"\n",
        "    n = 5\n",
        "    return n"
      ],
      "metadata": {
        "id": "dnQnaCJXqXzH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ],
      "metadata": {
        "id": "hD8UJJ2plj5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tokens):\n",
        "  model.total_unigrams = len(tokens)\n",
        "  for n in range(1, model.n + 1):\n",
        "    counts = model.ngram_counts[n - 1]\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "      context = tuple(tokens[i:i + n - 1])\n",
        "      next_token = tokens[i + n - 1]\n",
        "      if context not in counts:\n",
        "        counts[context] = defaultdict(int)\n",
        "      counts[context][next_token] = counts[context][next_token] + 1\n"
      ],
      "metadata": {
        "id": "9kQxiEYMljB6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main model training block\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize random seeds for reproducibility\n",
        "    set_seed(42)\n",
        "    n = get_hyperparameters()\n",
        "    model_name = \"count_model\"\n",
        "\n",
        "    # Download and prepare the Brown corpus\n",
        "    data_url = \"https://www.thelmbook.com/data/brown\"\n",
        "    train_corpus, test_corpus = download_and_prepare_data(data_url)\n",
        "\n",
        "    # Train the model and evaluate its performance\n",
        "    print(\"\\nTraining the model...\")\n",
        "    model = CountLanguageModel(n)\n",
        "    train(model, train_corpus)\n",
        "    print(\"\\nModel training complete.\")\n",
        "\n",
        "    perplexity = compute_perplexity(model, test_corpus, n)\n",
        "    print(f\"\\nPerplexity on test corpus: {perplexity:.2f}\")\n",
        "\n",
        "    save_model(model, model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwn0NRvPpMFZ",
        "outputId": "c80b4024-23b1-421b-fb29-c7d89c1cffaa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading corpus from https://www.thelmbook.com/data/brown...\n",
            "Decompressing and reading the corpus...\n",
            "Corpus size: 6185606 characters\n",
            "\n",
            "Training the model...\n",
            "\n",
            "Model training complete.\n",
            "\n",
            "Perplexity on test corpus: 299.06\n",
            "Saving model to models/count_model.pkl...\n",
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "xS59DtprrZOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main model testing block\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    model = load_model(model_name)\n",
        "\n",
        "    # Test the model with some example contexts\n",
        "    contexts = [\n",
        "        \"i will build a\",\n",
        "        \"the best place to\",\n",
        "        \"she was riding a\"\n",
        "    ]\n",
        "\n",
        "    # Generate completions for each context\n",
        "    for context in contexts:\n",
        "        tokens = tokenize(context)\n",
        "        next_token = model.predict_next_token(tokens)\n",
        "        print(f\"\\nContext: {context}\")\n",
        "        print(f\"Next token: {next_token}\")\n",
        "        print(f\"Generated text: {generate_text(model, tokens, 10)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ5xZ2PlrYL4",
        "outputId": "761b3cd7-183e-4d73-cb01-8cbcc3e57ec1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from models/count_model.pkl...\n",
            "Model loaded successfully.\n",
            "\n",
            "Context: i will build a\n",
            "Next token: wall\n",
            "Generated text: i will build a wall to keep the people in and added so long\n",
            "\n",
            "Context: the best place to\n",
            "Next token: live\n",
            "Generated text: the best place to live in 30 per cent to get happiness for yourself\n",
            "\n",
            "Context: she was riding a\n",
            "Next token: horse\n",
            "Generated text: she was riding a horse and showing a dog are very similar your aids\n"
          ]
        }
      ]
    }
  ]
}