{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsPuwTWYYiIS7ebz4+6kwa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simply-pouria/The-LMs-Book/blob/main/TheLMBook_Chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2\n",
        "### Training a Bag-of-Words Neural Network\n",
        "using the sofmax activation function and cross-entropy loss function\n"
      ],
      "metadata": {
        "id": "c0VyqBCdg72R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qppRH29kpfHo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.manual_seed(42)\n",
        "\n",
        "docs = [\n",
        " \"Movies are fun for everyone.\",\n",
        " \"Watching movies is great fun.\",\n",
        " \"Enjoy a great movie today.\",\n",
        " \"Research is interesting and important.\",\n",
        " \"Learning math is very important.\",\n",
        " \"Science discovery is interesting.\",\n",
        " \"Rock is great to listen to.\",\n",
        " \"Listen to music for fun.\",\n",
        " \"Music is fun for everyone.\",\n",
        " \"Listen to folk music!\"\n",
        "]\n",
        "labels = [1, 1, 1, 3, 3, 3, 2, 2, 2, 2]\n",
        "num_classes = len(set(labels))\n",
        "\n",
        "def tokenize(text):\n",
        "  return re.findall(r\"\\w+\", text.lower())\n",
        "def get_vocabulary(texts):\n",
        "  tokens = {token for text in texts for token in tokenize(text)}\n",
        "  return {word: idx for idx, word in enumerate(sorted(tokens))}\n",
        "\n",
        "vocabulary = get_vocabulary(docs)\n",
        "\n",
        "def doc_to_bow(doc, vocabulary):\n",
        "  tokens = set(tokenize(doc))\n",
        "  bow = [0] * len(vocabulary)\n",
        "  for token in tokens:\n",
        "    if token in vocabulary:\n",
        "      bow[vocabulary[token]] = 1\n",
        "      return bow\n",
        "\n",
        "vectors = torch.tensor(\n",
        " [doc_to_bow(doc, vocabulary) for doc in docs],\n",
        " dtype=torch.float32\n",
        ")\n",
        "labels = torch.tensor(labels, dtype=torch.long) - 1\n",
        "\n",
        "input_dim = len(vocabulary)\n",
        "hidden_dim = 50\n",
        "output_dim = num_classes\n",
        "\n",
        "class SimpleClassifier(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "model = SimpleClassifier(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for step in range(3000):\n",
        "  optimizer.zero_grad()\n",
        "  loss = criterion(model(vectors), labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inferencing the Trained Model"
      ],
      "metadata": {
        "id": "3mgyNa5-g66p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_docs = [\n",
        " \"Listening to rock music is fun.\",\n",
        " \"I like to listen to this song.\"\n",
        "]\n",
        "class_names = [\"Cinema\", \"Music\", \"Science\"]\n",
        "new_doc_vectors = torch.tensor(\n",
        " [doc_to_bow(new_doc, vocabulary) for new_doc in new_docs]\n",
        ",\n",
        " dtype=torch.float32\n",
        ")\n",
        "with torch.no_grad():\n",
        " outputs = model(new_doc_vectors)\n",
        " predicted_ids = torch.argmax(outputs, dim=1) + 1\n",
        "for i, new_doc in enumerate(new_docs):\n",
        " print(f'{new_doc}: {class_names[predicted_ids[i].item() -1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY2IWJkkpihx",
        "outputId": "48f8815e-afb9-43f3-e830-0a046cb07ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listening to rock music is fun.: Music\n",
            "I like to listen to this song.: Music\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementning Byte-Pair Encoding\n"
      ],
      "metadata": {
        "id": "y3Jj9Szqhajm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def initialize_vocabulary(corpus):\n",
        "  vocabulary = defaultdict(int)\n",
        "  charset = set()\n",
        "  for word in corpus:\n",
        "    word_with_marker = '_' + word\n",
        "    characters = list(word_with_marker)\n",
        "    charset.update(characters)\n",
        "    tokenized_word = ' '.join(characters)\n",
        "    vocabulary[tokenized_word] += 1\n",
        "  return vocabulary, charset\n",
        "\n",
        "\n",
        "def get_pair_counts(vocabulary):\n",
        "  pair_counts = defaultdict(int)\n",
        "  for tokenized_word, count in vocabulary.items():\n",
        "    tokens = tokenized_word.split()\n",
        "    for i in range(len(tokens) - 1):\n",
        "      pair = (tokens[i], tokens[i + 1])\n",
        "      pair_counts[pair] += count\n",
        "  return pair_counts\n",
        "\n",
        "\n",
        "def merge_pair(vocabulary, pair):\n",
        "  new_vocabulary = {}\n",
        "  bigram = re.escape(' '.join(pair))\n",
        "  pattern = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
        "  for tokenized_word, count in vocabulary.items():\n",
        "    new_tokenized_word = pattern.sub(\"\".join(pair), tokenized_word)\n",
        "    new_vocabulary[new_tokenized_word] = count\n",
        "  return new_vocabulary\n",
        "\n",
        "\n",
        "def byte_pair_encoding(corpus, vocab_size):\n",
        "  vocabulary, charset = initialize_vocabulary(corpus)\n",
        "  merges = []\n",
        "  tokens = set(charset)\n",
        "  while len(tokens) < vocab_size:\n",
        "    pair_counts = get_pair_counts(vocabulary)\n",
        "    if not pair_counts:\n",
        "     break\n",
        "    most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
        "    merges.append(most_frequent_pair)\n",
        "    vocabulary = merge_pair(vocabulary, most_frequent_pair)\n",
        "    new_token = ''.join(most_frequent_pair) ➎\n",
        "    tokens.add(new_token) ➏\n",
        "\n",
        "  return vocabulary, merges, charset, tokens\n",
        "\n",
        "def tokenize_word(word, merges, vocabulary, charset, unk_token=\"<UNK>\"):\n",
        "  word = '_' + word\n",
        "  if word in vocabulary:\n",
        "    return [word]\n",
        "  tokens = [char if char in charset else unk_token for char in word]\n",
        "\n",
        "  for left, right in merges:\n",
        "    i = 0\n",
        "    while i < len(tokens) - 1:\n",
        "      if tokens[i:i+2] == [left, right]:\n",
        "        tokens[i:i+2] = [left + right]\n",
        "      else:\n",
        "        i += 1\n",
        "  return tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "O4Plsz61h_eh",
        "outputId": "a4354d47-1534-48d5-ac0e-feafc08c22dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 33)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    def byte_pair_encoding(corpus, vocab_size):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    }
  ]
}